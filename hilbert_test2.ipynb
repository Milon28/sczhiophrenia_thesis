{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9a968f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Force immediate output\n",
    "sys.stdout.flush()\n",
    "os.environ['PYTHONUNBUFFERED'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6261752f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STARTING DEBUG VERSION ===\n",
      "Python version: 3.11.4 (tags/v3.11.4:d2340ef, Jun  7 2023, 05:45:37) [MSC v.1934 64 bit (AMD64)]\n",
      "PyTorch version: 2.7.1+cpu\n",
      "Using device: cpu\n",
      "\n",
      "=== MAIN FUNCTION STARTED ===\n",
      "Please update the folder paths in the script:\n",
      "SCHIZOPHRENIA_FOLDER = 'D:/HHT/S'\n",
      "HEALTHY_FOLDER = 'D:/HHT/H'\n",
      "\n",
      "Hyperparameters:\n",
      "  IMG_SIZE: 224\n",
      "  BATCH_SIZE: 8\n",
      "  LEARNING_RATE: 0.001\n",
      "  EPOCHS: 5\n",
      "  N_FOLDS: 2\n",
      "Data transforms created\n",
      "\n",
      "=== LOADING DATA ===\n",
      "Loading data from:\n",
      "  Schizophrenia folder: D:/HHT/S\n",
      "  Healthy folder: D:/HHT/H\n",
      "Found 5146 schizophrenia images\n",
      "Found 4235 healthy images\n",
      "Total loaded: 9381 images\n",
      "Data loaded successfully:\n",
      "  Total images: 9381\n",
      "  Schizophrenia: 5146 images\n",
      "  Healthy: 4235 images\n",
      "\n",
      "=== TESTING IMAGE LOADING ===\n",
      "Dataset created with 1 images\n",
      "Test image shape: torch.Size([3, 224, 224]), label: 1\n",
      "Image loading test passed ✓\n",
      "\n",
      "=== STARTING 2-FOLD CROSS VALIDATION ===\n",
      "\n",
      "============================================================\n",
      "FOLD 1/2\n",
      "============================================================\n",
      "Train set: 4690 images\n",
      "Val set: 4691 images\n",
      "Creating datasets...\n",
      "Dataset created with 4690 images\n",
      "Dataset created with 4691 images\n",
      "Creating data loaders...\n",
      "Train loader: 587 batches\n",
      "Val loader: 587 batches\n",
      "Initializing model...\n",
      "Simple ViT model created\n",
      "Loss function and optimizer created\n",
      "\n",
      "Starting training for 5 epochs...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "EPOCH 1/5 STARTING...\n",
      "  Starting training phase...\n",
      "    Processing batch 1, batch size: 8\n",
      "  Training completed: 587 batches processed\n",
      "Training completed - Loss: 0.6918, Acc: 54.07%\n",
      "  Starting validation phase...\n",
      "  Validation completed: 587 batches processed\n",
      "Validation completed - Loss: 0.6892, Acc: 54.85%\n",
      "✓ EPOCH [ 1/5] RESULTS:\n",
      "  Train: Loss=0.6918, Acc= 54.07%\n",
      "  Val:   Loss=0.6892, Acc= 54.85%, Prec= 30.08%, Rec= 54.85%, F1= 38.86%\n",
      "\n",
      "EPOCH 2/5 STARTING...\n",
      "  Starting training phase...\n",
      "    Processing batch 1, batch size: 8\n",
      "  Training completed: 587 batches processed\n",
      "Training completed - Loss: 0.6896, Acc: 54.73%\n",
      "  Starting validation phase...\n",
      "  Validation completed: 587 batches processed\n",
      "Validation completed - Loss: 0.6905, Acc: 54.85%\n",
      "✓ EPOCH [ 2/5] RESULTS:\n",
      "  Train: Loss=0.6896, Acc= 54.73%\n",
      "  Val:   Loss=0.6905, Acc= 54.85%, Prec= 30.08%, Rec= 54.85%, F1= 38.86%\n",
      "\n",
      "EPOCH 3/5 STARTING...\n",
      "  Starting training phase...\n",
      "    Processing batch 1, batch size: 8\n",
      "  Training completed: 587 batches processed\n",
      "Training completed - Loss: 0.6887, Acc: 54.86%\n",
      "  Starting validation phase...\n",
      "  Validation completed: 587 batches processed\n",
      "Validation completed - Loss: 0.6881, Acc: 54.85%\n",
      "✓ EPOCH [ 3/5] RESULTS:\n",
      "  Train: Loss=0.6887, Acc= 54.86%\n",
      "  Val:   Loss=0.6881, Acc= 54.85%, Prec= 30.08%, Rec= 54.85%, F1= 38.86%\n",
      "\n",
      "EPOCH 4/5 STARTING...\n",
      "  Starting training phase...\n",
      "    Processing batch 1, batch size: 8\n",
      "  Training completed: 587 batches processed\n",
      "Training completed - Loss: 0.6890, Acc: 54.63%\n",
      "  Starting validation phase...\n",
      "  Validation completed: 587 batches processed\n",
      "Validation completed - Loss: 0.6886, Acc: 54.85%\n",
      "✓ EPOCH [ 4/5] RESULTS:\n",
      "  Train: Loss=0.6890, Acc= 54.63%\n",
      "  Val:   Loss=0.6886, Acc= 54.85%, Prec= 30.08%, Rec= 54.85%, F1= 38.86%\n",
      "\n",
      "EPOCH 5/5 STARTING...\n",
      "  Starting training phase...\n",
      "    Processing batch 1, batch size: 8\n",
      "  Training completed: 587 batches processed\n",
      "Training completed - Loss: 0.6880, Acc: 54.86%\n",
      "  Starting validation phase...\n",
      "  Validation completed: 587 batches processed\n",
      "Validation completed - Loss: 0.6874, Acc: 54.85%\n",
      "✓ EPOCH [ 5/5] RESULTS:\n",
      "  Train: Loss=0.6880, Acc= 54.86%\n",
      "  Val:   Loss=0.6874, Acc= 54.85%, Prec= 30.08%, Rec= 54.85%, F1= 38.86%\n",
      "\n",
      "Fold 1 completed successfully!\n",
      "\n",
      "=== DEBUG VERSION COMPLETED ===\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, precision_score, recall_score, f1_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Force immediate output\n",
    "sys.stdout.flush()\n",
    "os.environ['PYTHONUNBUFFERED'] = '1'\n",
    "\n",
    "print(\"=== STARTING DEBUG VERSION ===\")\n",
    "print(\"Python version:\", sys.version)\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Dataset class for loading HHT plot images\n",
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        print(f\"Dataset created with {len(image_paths)} images\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            image_path = self.image_paths[idx]\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            label = self.labels[idx]\n",
    "            \n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            \n",
    "            return image, label\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {image_path}: {e}\")\n",
    "            raise e\n",
    "\n",
    "# Simplified Vision Transformer for debugging\n",
    "class SimpleViT(nn.Module):\n",
    "    def __init__(self, img_size=224, n_classes=2):\n",
    "        super().__init__()\n",
    "        # Much simpler model for debugging\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, n_classes)\n",
    "        )\n",
    "        print(\"Simple ViT model created\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Data loading function with debug prints\n",
    "def load_data(schizophrenia_folder, healthy_folder):\n",
    "    \"\"\"Load image paths and labels from folders\"\"\"\n",
    "    print(f\"Loading data from:\")\n",
    "    print(f\"  Schizophrenia folder: {schizophrenia_folder}\")\n",
    "    print(f\"  Healthy folder: {healthy_folder}\")\n",
    "    \n",
    "    if not os.path.exists(schizophrenia_folder):\n",
    "        print(f\"ERROR: Schizophrenia folder does not exist: {schizophrenia_folder}\")\n",
    "        return [], []\n",
    "    \n",
    "    if not os.path.exists(healthy_folder):\n",
    "        print(f\"ERROR: Healthy folder does not exist: {healthy_folder}\")\n",
    "        return [], []\n",
    "    \n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    \n",
    "    # Load schizophrenia images (label = 1)\n",
    "    schiz_files = [f for f in os.listdir(schizophrenia_folder) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    print(f\"Found {len(schiz_files)} schizophrenia images\")\n",
    "    \n",
    "    for img_name in schiz_files:\n",
    "        image_paths.append(os.path.join(schizophrenia_folder, img_name))\n",
    "        labels.append(1)\n",
    "    \n",
    "    # Load healthy images (label = 0)\n",
    "    healthy_files = [f for f in os.listdir(healthy_folder) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    print(f\"Found {len(healthy_files)} healthy images\")\n",
    "    \n",
    "    for img_name in healthy_files:\n",
    "        image_paths.append(os.path.join(healthy_folder, img_name))\n",
    "        labels.append(0)\n",
    "    \n",
    "    print(f\"Total loaded: {len(image_paths)} images\")\n",
    "    return image_paths, labels\n",
    "\n",
    "# Training function with debug prints\n",
    "def train_model(model, train_loader, criterion, optimizer, device):\n",
    "    print(\"  Starting training phase...\")\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    batch_count = 0\n",
    "    for images, labels in train_loader:\n",
    "        batch_count += 1\n",
    "        if batch_count == 1:\n",
    "            print(f\"    Processing batch {batch_count}, batch size: {images.size(0)}\")\n",
    "        \n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_acc = 100 * correct / total\n",
    "    print(f\"  Training completed: {batch_count} batches processed\")\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "# Validation function with debug prints\n",
    "def validate_model(model, val_loader, criterion, device):\n",
    "    print(\"  Starting validation phase...\")\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    batch_count = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            batch_count += 1\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    epoch_loss = running_loss / len(val_loader)\n",
    "    epoch_acc = 100 * correct / total\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    precision = precision_score(all_labels, all_predictions, average='weighted', zero_division=0) * 100\n",
    "    recall = recall_score(all_labels, all_predictions, average='weighted', zero_division=0) * 100\n",
    "    f1 = f1_score(all_labels, all_predictions, average='weighted', zero_division=0) * 100\n",
    "    \n",
    "    print(f\"  Validation completed: {batch_count} batches processed\")\n",
    "    return epoch_loss, epoch_acc, precision, recall, f1, all_predictions, all_labels\n",
    "\n",
    "# Main function with extensive debugging\n",
    "def main():\n",
    "    print(\"\\n=== MAIN FUNCTION STARTED ===\")\n",
    "    \n",
    "    # Configure these paths according to your data structure\n",
    "    SCHIZOPHRENIA_FOLDER = \"D:/HHT/S\"  # Update this path\n",
    "    HEALTHY_FOLDER = \"D:/HHT/H\"  # Update this path\n",
    "    \n",
    "    print(\"Please update the folder paths in the script:\")\n",
    "    print(f\"SCHIZOPHRENIA_FOLDER = '{SCHIZOPHRENIA_FOLDER}'\")\n",
    "    print(f\"HEALTHY_FOLDER = '{HEALTHY_FOLDER}'\")\n",
    "    \n",
    "    # For testing, let's create some dummy data if folders don't exist\n",
    "    if not os.path.exists(SCHIZOPHRENIA_FOLDER) or not os.path.exists(HEALTHY_FOLDER):\n",
    "        print(\"\\n⚠️  FOLDERS NOT FOUND - CANNOT PROCEED\")\n",
    "        print(\"Please update the folder paths and run again\")\n",
    "        return\n",
    "    \n",
    "    # Hyperparameters\n",
    "    IMG_SIZE = 224\n",
    "    BATCH_SIZE = 8  # Smaller batch size for debugging\n",
    "    LEARNING_RATE = 1e-3\n",
    "    EPOCHS = 5  # Fewer epochs for debugging\n",
    "    N_FOLDS = 2  # Fewer folds for debugging\n",
    "    \n",
    "    print(f\"\\nHyperparameters:\")\n",
    "    print(f\"  IMG_SIZE: {IMG_SIZE}\")\n",
    "    print(f\"  BATCH_SIZE: {BATCH_SIZE}\")\n",
    "    print(f\"  LEARNING_RATE: {LEARNING_RATE}\")\n",
    "    print(f\"  EPOCHS: {EPOCHS}\")\n",
    "    print(f\"  N_FOLDS: {N_FOLDS}\")\n",
    "    \n",
    "    # Data transforms\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    print(\"Data transforms created\")\n",
    "    \n",
    "    # Load data\n",
    "    print(\"\\n=== LOADING DATA ===\")\n",
    "    image_paths, labels = load_data(SCHIZOPHRENIA_FOLDER, HEALTHY_FOLDER)\n",
    "    \n",
    "    if len(image_paths) == 0:\n",
    "        print(\"ERROR: No images loaded. Check your folder paths!\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Data loaded successfully:\")\n",
    "    print(f\"  Total images: {len(image_paths)}\")\n",
    "    print(f\"  Schizophrenia: {sum(labels)} images\")\n",
    "    print(f\"  Healthy: {len(labels) - sum(labels)} images\")\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    image_paths = np.array(image_paths)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    # Test loading one image\n",
    "    print(\"\\n=== TESTING IMAGE LOADING ===\")\n",
    "    try:\n",
    "        test_dataset = EEGDataset([image_paths[0]], [labels[0]], transform=transform)\n",
    "        test_img, test_label = test_dataset[0]\n",
    "        print(f\"Test image shape: {test_img.shape}, label: {test_label}\")\n",
    "        print(\"Image loading test passed ✓\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Image loading test failed: {e}\")\n",
    "        return\n",
    "    \n",
    "    # 2-fold cross validation for debugging\n",
    "    print(f\"\\n=== STARTING {N_FOLDS}-FOLD CROSS VALIDATION ===\")\n",
    "    skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=42)\n",
    "    fold_results = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(image_paths, labels)):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"FOLD {fold + 1}/{N_FOLDS}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Split data\n",
    "        train_paths, val_paths = image_paths[train_idx], image_paths[val_idx]\n",
    "        train_labels, val_labels = labels[train_idx], labels[val_idx]\n",
    "        \n",
    "        print(f\"Train set: {len(train_paths)} images\")\n",
    "        print(f\"Val set: {len(val_paths)} images\")\n",
    "        \n",
    "        # Create datasets\n",
    "        print(\"Creating datasets...\")\n",
    "        train_dataset = EEGDataset(train_paths, train_labels, transform=transform)\n",
    "        val_dataset = EEGDataset(val_paths, val_labels, transform=transform)\n",
    "        \n",
    "        # Create data loaders\n",
    "        print(\"Creating data loaders...\")\n",
    "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)  # num_workers=0 for debugging\n",
    "        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "        \n",
    "        print(f\"Train loader: {len(train_loader)} batches\")\n",
    "        print(f\"Val loader: {len(val_loader)} batches\")\n",
    "        \n",
    "        # Initialize model\n",
    "        print(\"Initializing model...\")\n",
    "        model = SimpleViT(img_size=IMG_SIZE, n_classes=2).to(device)\n",
    "        \n",
    "        # Loss and optimizer\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
    "        print(\"Loss function and optimizer created\")\n",
    "        \n",
    "        # Training loop\n",
    "        print(f\"\\nStarting training for {EPOCHS} epochs...\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        for epoch in range(EPOCHS):\n",
    "            print(f\"\\nEPOCH {epoch+1}/{EPOCHS} STARTING...\")\n",
    "            \n",
    "            try:\n",
    "                # Training phase\n",
    "                train_loss, train_acc = train_model(model, train_loader, criterion, optimizer, device)\n",
    "                print(f\"Training completed - Loss: {train_loss:.4f}, Acc: {train_acc:.2f}%\")\n",
    "                \n",
    "                # Validation phase  \n",
    "                val_loss, val_acc, val_precision, val_recall, val_f1, val_predictions, val_true = validate_model(model, val_loader, criterion, device)\n",
    "                print(f\"Validation completed - Loss: {val_loss:.4f}, Acc: {val_acc:.2f}%\")\n",
    "                \n",
    "                # THIS IS THE MAIN RESULT LINE - SHOULD ALWAYS PRINT\n",
    "                print(f'✓ EPOCH [{epoch+1:2d}/{EPOCHS}] RESULTS:')\n",
    "                print(f'  Train: Loss={train_loss:.4f}, Acc={train_acc:6.2f}%')\n",
    "                print(f'  Val:   Loss={val_loss:.4f}, Acc={val_acc:6.2f}%, Prec={val_precision:6.2f}%, Rec={val_recall:6.2f}%, F1={val_f1:6.2f}%')\n",
    "                \n",
    "                # Force output flush\n",
    "                sys.stdout.flush()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"ERROR in epoch {epoch+1}: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                return\n",
    "        \n",
    "        print(f\"\\nFold {fold + 1} completed successfully!\")\n",
    "        break  # Only do first fold for debugging\n",
    "    \n",
    "    print(\"\\n=== DEBUG VERSION COMPLETED ===\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except Exception as e:\n",
    "        print(f\"CRITICAL ERROR: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
